# ----------------------
# 환경 설정 (Env Config)
# ----------------------
# 사용 가능한 환경 예시:
# env_name: "Walker2d-v5"      # 2족 보행, 중간 난이도
env_name: "InvertedPendulum-v5"  # 가장 쉬운 환경 (막대 세우기)
# env_name: "CartPole-v1"     # 이산적 액션 환경 (Discrete Action)
# env_name: "MountainCar-v0"   # 연속적인 목표 설정 환경
# env_name: "HalfCheetah-v5"   # 난이도 높은 MuJoCo 환경
# env_name: "Hopper-v5"        # 현재 사용 중인 환경

# ----------------------
# 모델 및 학습 알고리즘 설정 (Model & Training Method)
# ----------------------
model: "A2C"        # 사용 중인 강화학습 알고리즘 (Advantage Actor-Critic)
method: "TD_lambda" # Advantage 기반 학습 방식 사용

# ----------------------
# 하이퍼파라미터 설정 (Hyperparameters)
# ----------------------
lambda: 0.95          # 기존: 0.99 → GAE 감쇠 계수를 낮춰 고착된 정책 방지 및 빠른 적응 유도
batch_size: 16            # 기존: 128 → 더 안정적인 업데이트를 위해 배치 크기 증가
buffer_size: 200000   # A2C에서는 사용되지 않지만, 다른 방법과 호환 가능

# ----------------------
# 학습률 및 최적화 관련 설정 (Learning Rate & Optimization)
# ----------------------
lr: 0.0003            # 기존: 0.0001 → 너무 작은 학습률로 인해 수렴 속도가 느릴 가능성이 있어 증가
gamma: 0.99           # 할인 계수 (미래 보상의 가중치 조절, 변경 없음)
entropy_coeff: 0.02   # 기존: 0.01 → 초기 탐색을 보다 적극적으로 유도

# ----------------------
# Epsilon-Greedy Exploration (탐색 설정)
# ----------------------
epsilon_init: 1.0     # 초기 탐색 확률 (변경 없음)
epsilon_min: 0.05     # 기존: 0.1 → 더 다양한 탐색을 위해 최소 탐색 확률 감소
epsilon_decay: 0.997  # 기존: 0.995 → 탐색이 너무 빨리 줄어드는 것을 방지

# ----------------------
# 학습 단계 및 업데이트 설정 (Training Steps & Updates)
# ----------------------
n_step: 5000000         # 총 학습 스텝 수 (변경 없음)
n_train_start: 5000     # 기존: 2000 → 충분한 경험을 쌓고 학습 시작
# target_update_freq: 1000  # A2C에서는 타겟 네트워크 업데이트가 필요 없음 → 제거

# ----------------------
# 평가 및 테스트 주기 (Evaluation & Testing)
# ----------------------
test_freq: 10000        # 기존: 5000 → 평가 빈도를 줄여 과적합 방지
total_episodes: 30000   # 총 에피소드 수 (변경 없음)


