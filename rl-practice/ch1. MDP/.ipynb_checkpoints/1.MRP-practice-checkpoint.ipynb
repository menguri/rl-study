{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e6c50a-3537-47a0-9394-e0a4d359b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1697e1-e5f4-4dc6-a24b-d8a951a03604",
   "metadata": {},
   "source": [
    "## **1. MRP 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f10428-db17-4c3e-a4f8-5987a72a7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#상태 전이 확률을 정의.\n",
    "#각 state와, state에서 전이 가능한 state, 전이할 확률을 순서대로 정의\n",
    "\n",
    "transition_matrix = {\n",
    "    'facebook': {     #state facebook은 다음에 90% 확률로 facebook, 10%의 확률로 class 1으로 이동\n",
    "        'facebook': 0.9,\n",
    "        'class 1': 0.1\n",
    "    },\n",
    "    'class 1': {      #state class 1은 다음에 50% 확률로 facebook, 50%의 확률로 class 2로 이동\n",
    "        'facebook': 0.5,\n",
    "        'class 2': 0.5\n",
    "    },\n",
    "    'class 2': {       #state class 2은 다음에 20% 확률로 sleep, 80%의 확률로 class 3으로 이동\n",
    "        'sleep': 0.2,\n",
    "        'class 3': 0.8\n",
    "    },\n",
    "    'class 3': {\n",
    "        'pub': 0.4,\n",
    "        'pass': 0.6\n",
    "    },\n",
    "    'pub': {\n",
    "            'class 1': 0.2,\n",
    "            'class 2': 0.4,\n",
    "            'class 3': 0.4,\n",
    "        },\n",
    "    'pass': {         # pass state는 반드시 sleep 1으로 이동\n",
    "        'sleep': 1\n",
    "    },\n",
    "    'sleep': {'sleep': 1}   #sleep은 sleep state만으로 이동하며, terminal state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecd658c5-1e03-425b-bf46-4391082fb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    return {\n",
    "        'facebook': -1,\n",
    "        'class 1': -2,\n",
    "        'class 2': -2,\n",
    "        'class 3': -2, \n",
    "        'pub': 1,\n",
    "        'pass': 10,\n",
    "        'sleep': 0,\n",
    "    }[state]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a064de6-13ff-4aeb-810c-18ff1114d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_choice(choices):\n",
    "    values, probs = zip(*choices)\n",
    "    value = np.random.choice(values, size=1, replace=True, p=probs)\n",
    "\n",
    "    return value[0]\n",
    "\n",
    "def Markov_Rewrad_Process(state, gamma):\n",
    "\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    \n",
    "    while state != 'sleep':                       #에피소드가 끝날 때 까지 반복\n",
    "        # print(\"현재 상태: {}\".format(state))  # 현재 state를 출력\n",
    "\n",
    "        total_reward += pow(gamma, t) * reward(state)\n",
    "        \n",
    "        next_state_prob = transition_matrix[state]  #현재 상태에서 전이 가능한 state와 확률을 가져옴\n",
    "        # print(next_state_prob)\n",
    "        \n",
    "        next_state = stochastic_choice(next_state_prob.items()) #상태 전이 확률 정보를 통해 다음 state 결정\n",
    "\n",
    "        # print(\"다음 상태: {}\".format(next_state))         #선택된 다음 상태 출력\n",
    "\n",
    "        state = next_state                         #다음 상태를 현재 상태로 입력하고, 반복\n",
    "        t += 1\n",
    "        # print()\n",
    "\n",
    "    # print(\"Agent go to sleeep. Episode Done!\")\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff99bb23-dc65-4aa5-9354-f4781c2a044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_state_value_mrp(state, gamma, episode):\n",
    "\n",
    "    reward_list = []\n",
    "\n",
    "    for t in tqdm.tqdm(range(episode)):\n",
    "        re_t = Markov_Rewrad_Process(state, gamma)\n",
    "        reward_list.append(re_t)\n",
    "\n",
    "    # print(sum(reward_list))\n",
    "    return round(sum(reward_list) / episode, 2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e666570a-e2c7-4ea6-98e8-f0efd5f2785a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 4456.18it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 2068.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 32707.31it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 7993.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 7036.18it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 3403086.41it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 400/400 [00:00<00:00, 1472.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pub': 2.04,\n",
       " 'class 1': -4.77,\n",
       " 'pass': 10.0,\n",
       " 'class 2': 1.01,\n",
       " 'class 3': 4.01,\n",
       " 'sleep': 0.0,\n",
       " 'facebook': -7.36}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_value = {key:0 for key in set(transition_matrix.keys())}\n",
    "for target_state in set(transition_matrix.keys()):\n",
    "    s_v = calculate_state_value_mrp(target_state, 0.9, 400)\n",
    "    state_value[target_state] = s_v\n",
    "\n",
    "state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e27c4-f598-4a44-b897-e57f3c1e7243",
   "metadata": {},
   "source": [
    "## **2. Gamma Experiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc872dfd-882c-4ff5-bcfc-692c5cd321d8",
   "metadata": {},
   "source": [
    "### **2-1. State, Action, Reward, Transition prob def**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206915f5-d9de-403a-81d2-4a3e54ad8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 행동은 상태전이확률과 비슷하게 현재 상태에서 수행 가능한 행동으로 표현\n",
    "actions = {\n",
    "    'facebook': ['continue', 'go to class'],\n",
    "    'class 1': ['checkout facebook', 'study'],\n",
    "    'class 2': ['sleep', 'study'],\n",
    "    'class 3': ['go to the pub', 'study'],\n",
    "    'pub': [\"return to study\"],\n",
    "    'sleep': ['sleep'],\n",
    "}\n",
    "\n",
    "#action의 key에 따라 states 재정의\n",
    "states = set(actions.keys())  #state를 다시 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af352b0-0ac1-43b3-9210-ecb318c372d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 전이확률은 이제 행동의 영향을 함께 받기 때문에, 다시 정의할 필요가 있음\n",
    "def transition_prob(state, action):\n",
    "    '''transition probabilities given state and action'''\n",
    "    return {\n",
    "        'facebook': {\n",
    "            'continue': {'facebook': 1},\n",
    "            'go to class': {'class 1': 1}\n",
    "        },\n",
    "        'class 1': {\n",
    "            'checkout facebook': {'facebook': 1},\n",
    "            'study': {'class 2': 1}\n",
    "        },\n",
    "    #class2는 sleep = {sleep, 1}, study = {class 3, 1} 의 전이 확률을 가짐\n",
    "    'class 2': {\n",
    "            'sleep': {'sleep': 1},\n",
    "            'study': {'class 3': 1}\n",
    "        },\n",
    "    #class3는 go to the pub= {pub, 1}, study={sleep, 1}의 전이 확률을 가짐\n",
    "    'class 3': {\n",
    "            'go to the pub': {'pub': 1},\n",
    "            'study': {'sleep': 1}\n",
    "        },\n",
    "    #pub은 return to study={class 1: 0.2 , class 2 : 0.4, class 3 : 0.4}의 전이 확률을 가짐\n",
    "    'pub': {\n",
    "            \"return to study\" : {\n",
    "                'class 1': 0.2,\n",
    "                'class 2': 0.4,\n",
    "                'class 3': 0.4,\n",
    "            }\n",
    "        },\n",
    "    #sleep은 sleep=1의 transition을 가짐\n",
    "    'sleep': {'sleep': {'sleep': 1}},\n",
    "}[state][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20e7c8ac-0a11-405f-b817-c92d21745ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state, action):\n",
    "    return {\n",
    "        'facebook': {'continue': -1, 'go to class': 0},\n",
    "        'class 1': {'checkout facebook': -1, 'study': -2},\n",
    "        'class 2': {'sleep': 0, 'study': -2},\n",
    "        'class 3': {'go to the pub': 1, 'study': 10},\n",
    "        'pub': {\"return to study\" : 0},\n",
    "        'sleep': {'sleep': 0},\n",
    "    }[state][action]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e63970-2502-4e26-ac65-cf3b2a7a4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MDP의 구성요소들을 입력시켜 mdu_tuple 생성\n",
    "mdp_tuple = (states, actions, transition_prob, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4c8812-0a6f-445a-8907-f46bd876322d",
   "metadata": {},
   "source": [
    "### **2-2. Value Iteration Model 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df322268-73e5-4b11-87e7-ef3ed999433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = {'facebook': 'go to class',\n",
    "               'class 3': 'study',\n",
    "               'class 2': 'study',\n",
    "               'sleep': 'sleep',\n",
    "               'pub': 'return to study',\n",
    "               'class 1': 'study'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bc5ba68-d547-4e9a-98b7-2920b7ce00d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp_tuple(함수 입력값) ({'pub', 'class 1', 'class 2', 'class 3', 'sleep', 'facebook'}, {'facebook': ['continue', 'go to class'], 'class 1': ['checkout facebook', 'study'], 'class 2': ['sleep', 'study'], 'class 3': ['go to the pub', 'study'], 'pub': ['return to study'], 'sleep': ['sleep']}, <function transition_prob at 0x7f14ba17c2c0>, <function reward at 0x7f14ba17c0e0>)\n",
      "Value_Iteration_0:{'pub': 0.0, 'class 1': -1.0, 'class 2': 0.0, 'class 3': 10.0, 'sleep': 0.0, 'facebook': -0.9}\n",
      "====================================================================================================\n",
      "Value_Iteration_1:{'pub': 3.4200000000000004, 'class 1': -1.81, 'class 2': 7.0, 'class 3': 10.0, 'sleep': 0.0, 'facebook': -1.629}\n",
      "====================================================================================================\n",
      "Value_Iteration_2:{'pub': 5.794200000000001, 'class 1': 4.3, 'class 2': 7.0, 'class 3': 10.0, 'sleep': 0.0, 'facebook': 3.87}\n",
      "====================================================================================================\n",
      "Value_Iteration_3:{'pub': 6.894000000000001, 'class 1': 4.3, 'class 2': 7.0, 'class 3': 10.0, 'sleep': 0.0, 'facebook': 3.87}\n",
      "====================================================================================================\n",
      "Value_Iteration_4:{'pub': 6.894000000000001, 'class 1': 4.3, 'class 2': 7.0, 'class 3': 10.0, 'sleep': 0.0, 'facebook': 3.87}\n",
      "====================================================================================================\n",
      "최종 가치 함수(함수 출력값): {'pub': 6.894000000000001, 'class 1': 4.3, 'class 2': 7.0, 'class 3': 10.0, 'sleep': 0.0, 'facebook': 3.87}\n"
     ]
    }
   ],
   "source": [
    "#함수를 통해 정의하면 쉽게 확인 가능\n",
    "def value_iteration(mdp_tuple, steps=1, gamma = 0.9):\n",
    "    states, actions, transition_prob, reward = mdp_tuple    #입력받은 mdp_tuple을 분리하여 정의\n",
    "\n",
    "    # state value initialize\n",
    "    value = {s: 0 for s in states}    #value iteration 시작 전 value값 초기화\n",
    "    for i in range(steps):            #설정한 반복 횟수동안 value iteration 진행\n",
    "        for s, v in value.items():    #모든 상태에 대해 반복\n",
    "            rewards_in_s = []\n",
    "            for a in actions[s]:      #현재 선택된 상태에서 수행 가능한 행동에 대한 보상 계산\n",
    "                r = reward(s, a)\n",
    "                reward_for_a = r\n",
    "                for s_bar, p_ss in transition_prob(s, a).items(): #실제 보상에 이전 iteration으로 얻은 가치 함수의 값을 더해줌\n",
    "                    reward_for_a += gamma * p_ss * value[s_bar]\n",
    "                rewards_in_s.append(reward_for_a)\n",
    "            if len(rewards_in_s) > 0:       #만약 가치 값이 존재한다면, 그중 가장 큰 가치 값을 value로 선택\n",
    "                value[s] = max(rewards_in_s)\n",
    "\n",
    "        print(\"Value_Iteration_{}:{}\".format(i, value))\n",
    "        print(\"=\"*100)\n",
    "    return value\n",
    "\n",
    "mdp_tuple = (states, actions, transition_prob, reward)\n",
    "print(\"mdp_tuple(함수 입력값)\", mdp_tuple)\n",
    "value = value_iteration(mdp_tuple, steps=5)\n",
    "\n",
    "print(\"최종 가치 함수(함수 출력값):\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb86280-d5a1-42ee-b86a-c7ff2803e756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_from_value(state, action, value, prob, reward):   #학습된 value와 전이 확률, 보상에 대한 정보를 입력받음\n",
    "    rewards_action = []\n",
    "    for action in actions[state]:         # 현재 상태에서 선택 가능한 행동의 가치를 모두 계산\n",
    "        r = reward(state, action)\n",
    "        r_action = r\n",
    "        # print(state, action)\n",
    "        for s, p_ss in prob(state, action).items():\n",
    "            r_action += p_ss *  (reward(state, action) + value[s])\n",
    "        rewards_action.append((r_action, action))\n",
    "    return max(rewards_action)[1]     #현재 상태에서 가장 큰 가치를 가지는 행동을 선택. 결국 최적 행동을 선택하는 최적 정책이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "946b71e3-6fd7-43ed-a0c3-18b319828eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 MDP에서 정책 함수에 사용하기 위한 value 값 추가\n",
    "def MDP_VI(mdp_tuple, state, policy, value):\n",
    "    states, actions, prob, reward = mdp_tuple\n",
    "    gamma = 0.9\n",
    "\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "\n",
    "    while state != 'sleep':\n",
    "        print(\"current state: {}\".format(state))\n",
    "        ##############################코드 작성 ####################################\n",
    "\n",
    "        # 변경된 정책 함수와, 함수 값에 필요한 입력 데이터 설정\n",
    "        selected_action = policy_from_value(state, actions, value, prob, reward)\n",
    "\n",
    "\n",
    "        # 상태, 행동에 따른 전이 확률과, 전이 확률에 따라 다음 상태를 선택하는 함수 작성\n",
    "        next_state_probs = prob(state, selected_action)\n",
    "        next_state = stochastic_choice(next_state_probs.items())\n",
    "\n",
    "        # 보상함수 작성\n",
    "        r = reward(state, selected_action)\n",
    "\n",
    "\n",
    "        #다음 상태를 현재 상태로 변경, 누적 보상과 에피소드 수 증가\n",
    "        state = next_state\n",
    "        total_reward += r\n",
    "        k += 1\n",
    "        #############################################################################\n",
    "\n",
    "\n",
    "        print(\"action: {}\".format(selected_action))\n",
    "        print(\"next_state:\", end='');print(next_state)\n",
    "        print(\"reward: {}\".format(r))\n",
    "\n",
    "        print()\n",
    "    print(\"total reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eebe223a-c4a3-4a74-a99c-2796a79cb344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state: class 1\n",
      "action: study\n",
      "next_state:class 2\n",
      "reward: -2\n",
      "\n",
      "current state: class 2\n",
      "action: study\n",
      "next_state:class 3\n",
      "reward: -2\n",
      "\n",
      "current state: class 3\n",
      "action: study\n",
      "next_state:sleep\n",
      "reward: 10\n",
      "\n",
      "total reward: 6\n"
     ]
    }
   ],
   "source": [
    "# MDP(mdp_tuple, 'class 1', policy_from_value(state, actions, value, transition_prob, reward))\n",
    "value_v = value\n",
    "MDP_VI(mdp_tuple, 'class 1', policy_from_value, value_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d48389-0aee-4cf2-8b28-7333b58e9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_policy = {'facebook': 'go to class',\n",
    "               'class 3': 'study',\n",
    "               'class 2': 'study',\n",
    "               'sleep': 'sleep',\n",
    "               'pub': 'return to study',\n",
    "               'class 1': 'study'}\n",
    "\n",
    "def value_iteration(mdp_tuple, steps=1, gamma=0.9, best_policy = best_policy):\n",
    "    states, actions, transition_prob, reward = mdp_tuple\n",
    "    value = {s: 0 for s in states}  # 초기 가치 함수 설정\n",
    "    \n",
    "    start_time = time.time()\n",
    "    updated_states = 0  # 업데이트된 상태 개수\n",
    "    best_count = 0\n",
    "    \n",
    "    for i in range(steps):\n",
    "        new_value = value.copy()\n",
    "        for s in states:\n",
    "            rewards_in_s = []\n",
    "            for a in actions[s]:\n",
    "                r = reward(s, a)\n",
    "                reward_for_a = r + gamma * sum(p_ss * value[s_bar] for s_bar, p_ss in transition_prob(s, a).items())\n",
    "                rewards_in_s.append(reward_for_a)\n",
    "            \n",
    "            if rewards_in_s:\n",
    "                new_value[s] = max(rewards_in_s)\n",
    "                updated_states += 1\n",
    "\n",
    "        v_policy = {}\n",
    "        for state in states:\n",
    "            best_action = policy_from_value(state, actions, new_value, transition_prob, reward)\n",
    "            v_policy[state] = best_action\n",
    "        \n",
    "        if v_policy == best_policy:\n",
    "            if best_count > 1:\n",
    "                break # 가치 함수가 더 이상 변하지 않으면 종료\n",
    "            best_count += 1\n",
    "        value = new_value\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    return value, i + 1, elapsed_time, updated_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a2732bb-d9df-4234-b993-fced0de5a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gamma  Iterations  Time Taken (s)  Updated States\n",
      "0   0.10         200        0.011825            1200\n",
      "1   0.20         200        0.011791            1200\n",
      "2   0.30         200        0.011793            1200\n",
      "3   0.40           4        0.000239              24\n",
      "4   0.50           4        0.000237              24\n",
      "5   0.70           4        0.000241              24\n",
      "6   0.90           4        0.000236              24\n",
      "7   0.95           4        0.000235              24\n",
      "8   0.99           4        0.000236              24\n",
      "9   1.00           4        0.000244              24\n"
     ]
    }
   ],
   "source": [
    "# 감마값을 변경하며 실험\n",
    "gamma_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9, 0.95, 0.99, 1]\n",
    "num_steps = 200\n",
    "results = []\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    final_value, iterations, time_taken, updates = value_iteration(mdp_tuple, steps=num_steps, gamma=gamma)\n",
    "    results.append([gamma, iterations, time_taken, updates])\n",
    "\n",
    "# 데이터프레임 생성 및 출력\n",
    "df_results = pd.DataFrame(results, columns=[\"Gamma\", \"Iterations\", \"Time Taken (s)\", \"Updated States\"])\n",
    "print(df_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
