{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3OufLGHtmyu"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.23.5\n",
    "!pip install gym==0.25.2\n",
    "!pip install matplotlib\n",
    "!pip install gym[classic_control]\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVzYLEDLsOWv"
   },
   "source": [
    "목표: 알고리즘을 완성하고 높은 test score 성능 달성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUJIVfXFsPu3"
   },
   "source": [
    "수행 내용:\n",
    "1. 아래 Dueling DQN 코드에 10개의 빈칸 라인을 채워 코드를 구동시키세요.\n",
    "  - 빈칸 부분은 주석으로 표시되어 있으며 1줄의 라인을 채우시면 됩니다.\n",
    "2. 필요한 경우, 더 높은 성능을 달성하기 위해 hyperparameter조정, Q 네트워크 구조 변경 등 적절히 조절하세요.\n",
    "\n",
    "참고 사항:\n",
    "- 최종 점수는 마지막 10번의 test score 평균으로 계산합니다.\n",
    "- Acrobot 환경의 특성 상 test score는 음수가 나오게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361897,
     "status": "ok",
     "timestamp": 1704275499087,
     "user": {
      "displayName": "SangHyeon Lee",
      "userId": "16447616537935510715"
     },
     "user_tz": -540
    },
    "id": "DldwbCUzz_-1",
    "outputId": "916da00c-7da5-4d49-f982-1c4c89ae0520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[time:100]test score: -500.0, train loss: 0.005, epsilon: 0.100\n",
      "[time:200]test score: -500.0, train loss: 0.023, epsilon: 0.100\n",
      "[time:300]test score: -500.0, train loss: 0.038, epsilon: 0.100\n",
      "[time:400]test score: -500.0, train loss: 0.050, epsilon: 0.100\n",
      "[time:500]test score: -500.0, train loss: 0.012, epsilon: 0.100\n",
      "[time:600]test score: -500.0, train loss: 0.023, epsilon: 0.100\n",
      "[time:700]test score: -500.0, train loss: 0.026, epsilon: 0.100\n",
      "[time:800]test score: -500.0, train loss: 0.137, epsilon: 0.100\n",
      "[time:900]test score: -500.0, train loss: 0.297, epsilon: 0.100\n",
      "[time:1000]test score: -500.0, train loss: 0.049, epsilon: 0.099\n",
      "[time:1100]test score: -500.0, train loss: 0.161, epsilon: 0.099\n",
      "[time:1200]test score: -500.0, train loss: 0.092, epsilon: 0.099\n",
      "[time:1300]test score: -500.0, train loss: 0.099, epsilon: 0.099\n",
      "[time:1400]test score: -296.0, train loss: 0.146, epsilon: 0.099\n",
      "[time:1500]test score: -500.0, train loss: 0.349, epsilon: 0.099\n",
      "[time:1600]test score: -500.0, train loss: 0.196, epsilon: 0.099\n",
      "[time:1700]test score: -170.0, train loss: 0.611, epsilon: 0.099\n",
      "[time:1800]test score: -248.0, train loss: 0.175, epsilon: 0.098\n",
      "[time:1900]test score: -500.0, train loss: 0.231, epsilon: 0.098\n",
      "[time:2000]test score: -500.0, train loss: 1.124, epsilon: 0.098\n",
      "[time:2100]test score: -500.0, train loss: 0.312, epsilon: 0.098\n",
      "[time:2200]test score: -499.0, train loss: 0.158, epsilon: 0.098\n",
      "[time:2300]test score: -500.0, train loss: 0.332, epsilon: 0.098\n",
      "[time:2400]test score: -246.0, train loss: 0.275, epsilon: 0.097\n",
      "[time:2500]test score: -500.0, train loss: 0.182, epsilon: 0.097\n",
      "[time:2600]test score: -500.0, train loss: 0.330, epsilon: 0.097\n",
      "[time:2700]test score: -141.0, train loss: 0.462, epsilon: 0.097\n",
      "[time:2800]test score: -91.0, train loss: 0.419, epsilon: 0.097\n",
      "[time:2900]test score: -500.0, train loss: 0.358, epsilon: 0.096\n",
      "[time:3000]test score: -500.0, train loss: 0.660, epsilon: 0.096\n",
      "[time:3100]test score: -105.0, train loss: 0.819, epsilon: 0.096\n",
      "[time:3200]test score: -500.0, train loss: 5.147, epsilon: 0.096\n",
      "[time:3300]test score: -500.0, train loss: 0.451, epsilon: 0.096\n",
      "[time:3400]test score: -500.0, train loss: 0.581, epsilon: 0.095\n",
      "[time:3500]test score: -500.0, train loss: 0.431, epsilon: 0.095\n",
      "[time:3600]test score: -500.0, train loss: 0.225, epsilon: 0.095\n",
      "[time:3700]test score: -500.0, train loss: 0.526, epsilon: 0.095\n",
      "[time:3800]test score: -500.0, train loss: 1.308, epsilon: 0.095\n",
      "[time:3900]test score: -500.0, train loss: 0.351, epsilon: 0.095\n",
      "[time:4000]test score: -500.0, train loss: 1.791, epsilon: 0.095\n",
      "[time:4100]test score: -500.0, train loss: 0.417, epsilon: 0.094\n",
      "[time:4200]test score: -500.0, train loss: 0.386, epsilon: 0.094\n",
      "[time:4300]test score: -69.0, train loss: 0.303, epsilon: 0.094\n",
      "[time:4400]test score: -121.0, train loss: 0.448, epsilon: 0.093\n",
      "[time:4500]test score: -244.0, train loss: 0.485, epsilon: 0.093\n",
      "[time:4600]test score: -80.0, train loss: 0.600, epsilon: 0.093\n",
      "[time:4700]test score: -100.0, train loss: 0.309, epsilon: 0.093\n",
      "[time:4800]test score: -75.0, train loss: 0.548, epsilon: 0.092\n",
      "[time:4900]test score: -181.0, train loss: 0.333, epsilon: 0.092\n",
      "[time:5000]test score: -73.0, train loss: 0.665, epsilon: 0.091\n",
      "[time:5100]test score: -76.0, train loss: 0.514, epsilon: 0.091\n",
      "[time:5200]test score: -123.0, train loss: 0.410, epsilon: 0.090\n",
      "[time:5300]test score: -83.0, train loss: 0.427, epsilon: 0.090\n",
      "[time:5400]test score: -500.0, train loss: 0.389, epsilon: 0.090\n",
      "[time:5500]test score: -500.0, train loss: 1.913, epsilon: 0.090\n",
      "[time:5600]test score: -500.0, train loss: 0.746, epsilon: 0.089\n",
      "[time:5700]test score: -500.0, train loss: 0.634, epsilon: 0.089\n",
      "[time:5800]test score: -500.0, train loss: 0.457, epsilon: 0.089\n",
      "[time:5900]test score: -99.0, train loss: 0.636, epsilon: 0.088\n",
      "[time:6000]test score: -75.0, train loss: 0.800, epsilon: 0.088\n",
      "[time:6100]test score: -82.0, train loss: 0.377, epsilon: 0.087\n",
      "[time:6200]test score: -83.0, train loss: 1.204, epsilon: 0.087\n",
      "[time:6300]test score: -81.0, train loss: 1.209, epsilon: 0.086\n",
      "[time:6400]test score: -71.0, train loss: 0.438, epsilon: 0.086\n",
      "[time:6500]test score: -76.0, train loss: 0.608, epsilon: 0.086\n",
      "[time:6600]test score: -129.0, train loss: 0.489, epsilon: 0.086\n",
      "[time:6700]test score: -76.0, train loss: 0.571, epsilon: 0.085\n",
      "[time:6800]test score: -88.0, train loss: 0.523, epsilon: 0.085\n",
      "[time:6900]test score: -102.0, train loss: 0.574, epsilon: 0.084\n",
      "[time:7000]test score: -199.0, train loss: 0.806, epsilon: 0.084\n",
      "[time:7100]test score: -105.0, train loss: 0.948, epsilon: 0.083\n",
      "[time:7200]test score: -102.0, train loss: 0.442, epsilon: 0.083\n",
      "[time:7300]test score: -111.0, train loss: 0.557, epsilon: 0.083\n",
      "[time:7400]test score: -142.0, train loss: 3.755, epsilon: 0.083\n",
      "[time:7500]test score: -114.0, train loss: 0.456, epsilon: 0.082\n",
      "[time:7600]test score: -96.0, train loss: 0.626, epsilon: 0.081\n",
      "[time:7700]test score: -119.0, train loss: 0.820, epsilon: 0.081\n",
      "[time:7800]test score: -70.0, train loss: 0.693, epsilon: 0.081\n",
      "[time:7900]test score: -91.0, train loss: 0.451, epsilon: 0.080\n",
      "[time:8000]test score: -78.0, train loss: 0.460, epsilon: 0.080\n",
      "[time:8100]test score: -75.0, train loss: 0.906, epsilon: 0.079\n",
      "[time:8200]test score: -73.0, train loss: 0.601, epsilon: 0.079\n",
      "[time:8300]test score: -74.0, train loss: 0.405, epsilon: 0.079\n",
      "[time:8400]test score: -96.0, train loss: 0.730, epsilon: 0.079\n",
      "[time:8500]test score: -93.0, train loss: 0.743, epsilon: 0.078\n",
      "[time:8600]test score: -74.0, train loss: 0.466, epsilon: 0.078\n",
      "[time:8700]test score: -118.0, train loss: 0.478, epsilon: 0.077\n",
      "[time:8800]test score: -101.0, train loss: 0.572, epsilon: 0.077\n",
      "[time:8900]test score: -494.0, train loss: 0.942, epsilon: 0.076\n",
      "[time:9000]test score: -78.0, train loss: 0.533, epsilon: 0.076\n",
      "[time:9100]test score: -125.0, train loss: 1.244, epsilon: 0.076\n",
      "[time:9200]test score: -85.0, train loss: 0.698, epsilon: 0.075\n",
      "[time:9300]test score: -88.0, train loss: 2.338, epsilon: 0.075\n",
      "[time:9400]test score: -88.0, train loss: 0.527, epsilon: 0.074\n",
      "[time:9500]test score: -73.0, train loss: 0.616, epsilon: 0.074\n",
      "[time:9600]test score: -114.0, train loss: 3.005, epsilon: 0.074\n",
      "[time:9700]test score: -145.0, train loss: 0.706, epsilon: 0.073\n",
      "[time:9800]test score: -123.0, train loss: 0.688, epsilon: 0.073\n",
      "[time:9900]test score: -98.0, train loss: 0.793, epsilon: 0.073\n",
      "[time:10000]test score: -77.0, train loss: 0.618, epsilon: 0.072\n",
      "[time:10100]test score: -69.0, train loss: 1.228, epsilon: 0.072\n",
      "[time:10200]test score: -70.0, train loss: 0.964, epsilon: 0.071\n",
      "[time:10300]test score: -61.0, train loss: 0.447, epsilon: 0.071\n",
      "[time:10400]test score: -95.0, train loss: 0.481, epsilon: 0.071\n",
      "[time:10500]test score: -71.0, train loss: 0.548, epsilon: 0.070\n",
      "[time:10600]test score: -76.0, train loss: 0.533, epsilon: 0.070\n",
      "[time:10700]test score: -112.0, train loss: 0.526, epsilon: 0.069\n",
      "[time:10800]test score: -77.0, train loss: 0.643, epsilon: 0.069\n",
      "[time:10900]test score: -74.0, train loss: 0.839, epsilon: 0.069\n",
      "[time:11000]test score: -73.0, train loss: 1.082, epsilon: 0.068\n",
      "[time:11100]test score: -70.0, train loss: 0.805, epsilon: 0.068\n",
      "[time:11200]test score: -74.0, train loss: 0.893, epsilon: 0.068\n",
      "[time:11300]test score: -77.0, train loss: 0.963, epsilon: 0.067\n",
      "[time:11400]test score: -113.0, train loss: 0.770, epsilon: 0.067\n",
      "[time:11500]test score: -63.0, train loss: 0.468, epsilon: 0.067\n",
      "[time:11600]test score: -85.0, train loss: 0.984, epsilon: 0.066\n",
      "[time:11700]test score: -75.0, train loss: 0.596, epsilon: 0.066\n",
      "[time:11800]test score: -76.0, train loss: 0.547, epsilon: 0.066\n",
      "[time:11900]test score: -62.0, train loss: 1.110, epsilon: 0.065\n",
      "[time:12000]test score: -67.0, train loss: 0.502, epsilon: 0.065\n",
      "[time:12100]test score: -74.0, train loss: 0.774, epsilon: 0.064\n",
      "[time:12200]test score: -63.0, train loss: 0.409, epsilon: 0.064\n",
      "[time:12300]test score: -90.0, train loss: 0.586, epsilon: 0.064\n",
      "[time:12400]test score: -78.0, train loss: 0.376, epsilon: 0.063\n",
      "[time:12500]test score: -71.0, train loss: 0.576, epsilon: 0.063\n",
      "[time:12600]test score: -70.0, train loss: 0.676, epsilon: 0.063\n",
      "[time:12700]test score: -75.0, train loss: 1.168, epsilon: 0.062\n",
      "[time:12800]test score: -97.0, train loss: 0.942, epsilon: 0.062\n",
      "[time:12900]test score: -78.0, train loss: 0.361, epsilon: 0.061\n",
      "[time:13000]test score: -98.0, train loss: 0.463, epsilon: 0.061\n",
      "[time:13100]test score: -131.0, train loss: 0.467, epsilon: 0.061\n",
      "[time:13200]test score: -76.0, train loss: 0.818, epsilon: 0.061\n",
      "[time:13300]test score: -94.0, train loss: 0.503, epsilon: 0.060\n",
      "[time:13400]test score: -89.0, train loss: 1.052, epsilon: 0.060\n",
      "[time:13500]test score: -109.0, train loss: 0.354, epsilon: 0.059\n",
      "[time:13600]test score: -166.0, train loss: 0.443, epsilon: 0.059\n",
      "[time:13700]test score: -74.0, train loss: 0.934, epsilon: 0.059\n",
      "[time:13800]test score: -74.0, train loss: 0.417, epsilon: 0.058\n",
      "[time:13900]test score: -101.0, train loss: 0.564, epsilon: 0.058\n",
      "[time:14000]test score: -74.0, train loss: 0.487, epsilon: 0.058\n",
      "[time:14100]test score: -93.0, train loss: 0.586, epsilon: 0.058\n",
      "[time:14200]test score: -88.0, train loss: 0.533, epsilon: 0.057\n",
      "[time:14300]test score: -75.0, train loss: 0.504, epsilon: 0.057\n",
      "[time:14400]test score: -99.0, train loss: 1.811, epsilon: 0.057\n",
      "[time:14500]test score: -93.0, train loss: 0.636, epsilon: 0.056\n",
      "[time:14600]test score: -109.0, train loss: 0.428, epsilon: 0.056\n",
      "[time:14700]test score: -82.0, train loss: 0.951, epsilon: 0.056\n",
      "[time:14800]test score: -62.0, train loss: 0.583, epsilon: 0.056\n",
      "[time:14900]test score: -87.0, train loss: 0.656, epsilon: 0.056\n",
      "[time:15000]test score: -61.0, train loss: 0.774, epsilon: 0.056\n",
      "[time:15100]test score: -69.0, train loss: 0.765, epsilon: 0.056\n",
      "[time:15200]test score: -82.0, train loss: 0.503, epsilon: 0.055\n",
      "[time:15300]test score: -101.0, train loss: 0.623, epsilon: 0.055\n",
      "[time:15400]test score: -85.0, train loss: 0.605, epsilon: 0.055\n",
      "[time:15500]test score: -84.0, train loss: 0.204, epsilon: 0.055\n",
      "[time:15600]test score: -88.0, train loss: 0.445, epsilon: 0.054\n",
      "[time:15700]test score: -99.0, train loss: 0.569, epsilon: 0.054\n",
      "[time:15800]test score: -84.0, train loss: 0.458, epsilon: 0.054\n",
      "[time:15900]test score: -87.0, train loss: 0.584, epsilon: 0.053\n",
      "[time:16000]test score: -114.0, train loss: 0.652, epsilon: 0.053\n",
      "[time:16100]test score: -70.0, train loss: 0.453, epsilon: 0.053\n",
      "[time:16200]test score: -97.0, train loss: 0.568, epsilon: 0.052\n",
      "[time:16300]test score: -81.0, train loss: 0.726, epsilon: 0.052\n",
      "[time:16400]test score: -70.0, train loss: 0.472, epsilon: 0.052\n",
      "[time:16500]test score: -115.0, train loss: 0.492, epsilon: 0.052\n",
      "[time:16600]test score: -70.0, train loss: 0.702, epsilon: 0.052\n",
      "[time:16700]test score: -70.0, train loss: 0.530, epsilon: 0.051\n",
      "[time:16800]test score: -79.0, train loss: 0.586, epsilon: 0.051\n",
      "[time:16900]test score: -140.0, train loss: 0.531, epsilon: 0.051\n",
      "[time:17000]test score: -78.0, train loss: 0.680, epsilon: 0.050\n",
      "[time:17100]test score: -86.0, train loss: 0.480, epsilon: 0.050\n",
      "[time:17200]test score: -103.0, train loss: 0.686, epsilon: 0.050\n",
      "[time:17300]test score: -97.0, train loss: 0.634, epsilon: 0.049\n",
      "[time:17400]test score: -92.0, train loss: 0.545, epsilon: 0.049\n",
      "[time:17500]test score: -91.0, train loss: 1.178, epsilon: 0.049\n",
      "[time:17600]test score: -90.0, train loss: 0.518, epsilon: 0.049\n",
      "[time:17700]test score: -68.0, train loss: 0.949, epsilon: 0.048\n",
      "[time:17800]test score: -95.0, train loss: 0.576, epsilon: 0.048\n",
      "[time:17900]test score: -89.0, train loss: 0.828, epsilon: 0.048\n",
      "[time:18000]test score: -134.0, train loss: 1.080, epsilon: 0.047\n",
      "[time:18100]test score: -76.0, train loss: 0.836, epsilon: 0.047\n",
      "[time:18200]test score: -73.0, train loss: 0.387, epsilon: 0.047\n",
      "[time:18300]test score: -92.0, train loss: 0.761, epsilon: 0.046\n",
      "[time:18400]test score: -78.0, train loss: 0.394, epsilon: 0.046\n",
      "[time:18500]test score: -88.0, train loss: 0.468, epsilon: 0.046\n",
      "[time:18600]test score: -91.0, train loss: 0.698, epsilon: 0.046\n",
      "[time:18700]test score: -104.0, train loss: 0.379, epsilon: 0.045\n",
      "[time:18800]test score: -78.0, train loss: 0.574, epsilon: 0.045\n",
      "[time:18900]test score: -78.0, train loss: 0.462, epsilon: 0.045\n",
      "[time:19000]test score: -108.0, train loss: 0.569, epsilon: 0.044\n",
      "[time:19100]test score: -78.0, train loss: 0.470, epsilon: 0.044\n",
      "[time:19200]test score: -79.0, train loss: 0.364, epsilon: 0.044\n",
      "[time:19300]test score: -70.0, train loss: 0.214, epsilon: 0.044\n",
      "[time:19400]test score: -70.0, train loss: 0.560, epsilon: 0.044\n",
      "[time:19500]test score: -72.0, train loss: 0.539, epsilon: 0.043\n",
      "[time:19600]test score: -92.0, train loss: 0.479, epsilon: 0.043\n",
      "[time:19700]test score: -128.0, train loss: 0.639, epsilon: 0.043\n",
      "[time:19800]test score: -79.0, train loss: 0.401, epsilon: 0.042\n",
      "[time:19900]test score: -92.0, train loss: 0.558, epsilon: 0.042\n",
      "[time:20000]test score: -79.0, train loss: 0.387, epsilon: 0.042\n",
      "[time:20100]test score: -153.0, train loss: 0.701, epsilon: 0.042\n",
      "[time:20200]test score: -76.0, train loss: 0.436, epsilon: 0.042\n",
      "[time:20300]test score: -85.0, train loss: 0.600, epsilon: 0.041\n",
      "[time:20400]test score: -79.0, train loss: 0.399, epsilon: 0.041\n",
      "[time:20500]test score: -86.0, train loss: 0.389, epsilon: 0.041\n",
      "[time:20600]test score: -96.0, train loss: 0.441, epsilon: 0.041\n",
      "[time:20700]test score: -79.0, train loss: 0.188, epsilon: 0.040\n",
      "[time:20800]test score: -64.0, train loss: 0.364, epsilon: 0.040\n",
      "[time:20900]test score: -110.0, train loss: 0.522, epsilon: 0.040\n",
      "[time:21000]test score: -97.0, train loss: 0.313, epsilon: 0.040\n",
      "[time:21100]test score: -89.0, train loss: 0.367, epsilon: 0.039\n",
      "[time:21200]test score: -89.0, train loss: 0.379, epsilon: 0.039\n",
      "[time:21300]test score: -98.0, train loss: 0.549, epsilon: 0.039\n",
      "[time:21400]test score: -75.0, train loss: 0.653, epsilon: 0.039\n",
      "[time:21500]test score: -85.0, train loss: 0.520, epsilon: 0.039\n",
      "[time:21600]test score: -93.0, train loss: 0.311, epsilon: 0.038\n",
      "[time:21700]test score: -71.0, train loss: 0.511, epsilon: 0.038\n",
      "[time:21800]test score: -99.0, train loss: 0.511, epsilon: 0.038\n",
      "[time:21900]test score: -76.0, train loss: 0.401, epsilon: 0.037\n",
      "[time:22000]test score: -74.0, train loss: 0.424, epsilon: 0.037\n",
      "[time:22100]test score: -75.0, train loss: 0.408, epsilon: 0.037\n",
      "[time:22200]test score: -111.0, train loss: 0.746, epsilon: 0.037\n",
      "[time:22300]test score: -72.0, train loss: 0.571, epsilon: 0.037\n",
      "[time:22400]test score: -121.0, train loss: 0.340, epsilon: 0.037\n",
      "[time:22500]test score: -115.0, train loss: 0.785, epsilon: 0.036\n",
      "[time:22600]test score: -75.0, train loss: 0.714, epsilon: 0.036\n",
      "[time:22700]test score: -95.0, train loss: 0.286, epsilon: 0.036\n",
      "[time:22800]test score: -78.0, train loss: 0.288, epsilon: 0.036\n",
      "[time:22900]test score: -97.0, train loss: 0.733, epsilon: 0.035\n",
      "[time:23000]test score: -73.0, train loss: 0.595, epsilon: 0.035\n",
      "[time:23100]test score: -77.0, train loss: 0.570, epsilon: 0.035\n",
      "[time:23200]test score: -75.0, train loss: 0.463, epsilon: 0.035\n",
      "[time:23300]test score: -77.0, train loss: 0.650, epsilon: 0.035\n",
      "[time:23400]test score: -78.0, train loss: 0.550, epsilon: 0.034\n",
      "[time:23500]test score: -80.0, train loss: 0.434, epsilon: 0.034\n",
      "[time:23600]test score: -62.0, train loss: 0.448, epsilon: 0.034\n",
      "[time:23700]test score: -95.0, train loss: 0.399, epsilon: 0.034\n",
      "[time:23800]test score: -92.0, train loss: 0.404, epsilon: 0.034\n",
      "[time:23900]test score: -99.0, train loss: 0.447, epsilon: 0.033\n",
      "[time:24000]test score: -83.0, train loss: 0.341, epsilon: 0.033\n",
      "[time:24100]test score: -70.0, train loss: 1.146, epsilon: 0.033\n",
      "[time:24200]test score: -63.0, train loss: 0.358, epsilon: 0.033\n",
      "[time:24300]test score: -80.0, train loss: 0.287, epsilon: 0.033\n",
      "[time:24400]test score: -150.0, train loss: 0.467, epsilon: 0.033\n",
      "[time:24500]test score: -84.0, train loss: 0.525, epsilon: 0.032\n",
      "[time:24600]test score: -70.0, train loss: 0.440, epsilon: 0.032\n",
      "[time:24700]test score: -93.0, train loss: 0.659, epsilon: 0.032\n",
      "[time:24800]test score: -95.0, train loss: 3.205, epsilon: 0.032\n",
      "[time:24900]test score: -97.0, train loss: 0.795, epsilon: 0.032\n",
      "[time:25000]test score: -61.0, train loss: 0.687, epsilon: 0.031\n",
      "[time:25100]test score: -63.0, train loss: 0.366, epsilon: 0.031\n",
      "[time:25200]test score: -62.0, train loss: 0.446, epsilon: 0.031\n",
      "[time:25300]test score: -76.0, train loss: 0.359, epsilon: 0.031\n",
      "[time:25400]test score: -62.0, train loss: 0.336, epsilon: 0.031\n",
      "[time:25500]test score: -76.0, train loss: 0.506, epsilon: 0.030\n",
      "[time:25600]test score: -68.0, train loss: 0.656, epsilon: 0.030\n",
      "[time:25700]test score: -63.0, train loss: 1.013, epsilon: 0.030\n",
      "[time:25800]test score: -79.0, train loss: 0.365, epsilon: 0.030\n",
      "[time:25900]test score: -89.0, train loss: 0.480, epsilon: 0.030\n",
      "[time:26000]test score: -91.0, train loss: 0.576, epsilon: 0.030\n",
      "[time:26100]test score: -99.0, train loss: 0.375, epsilon: 0.029\n",
      "[time:26200]test score: -62.0, train loss: 0.387, epsilon: 0.029\n",
      "[time:26300]test score: -70.0, train loss: 0.469, epsilon: 0.029\n",
      "[time:26400]test score: -68.0, train loss: 0.664, epsilon: 0.029\n",
      "[time:26500]test score: -61.0, train loss: 0.345, epsilon: 0.029\n",
      "[time:26600]test score: -70.0, train loss: 0.333, epsilon: 0.029\n",
      "[time:26700]test score: -62.0, train loss: 0.345, epsilon: 0.028\n",
      "[time:26800]test score: -74.0, train loss: 0.949, epsilon: 0.028\n",
      "[time:26900]test score: -69.0, train loss: 0.372, epsilon: 0.028\n",
      "[time:27000]test score: -98.0, train loss: 0.633, epsilon: 0.028\n",
      "[time:27100]test score: -78.0, train loss: 0.383, epsilon: 0.028\n",
      "[time:27200]test score: -93.0, train loss: 0.530, epsilon: 0.027\n",
      "[time:27300]test score: -70.0, train loss: 0.512, epsilon: 0.027\n",
      "[time:27400]test score: -68.0, train loss: 0.520, epsilon: 0.027\n",
      "[time:27500]test score: -104.0, train loss: 0.244, epsilon: 0.027\n",
      "[time:27600]test score: -63.0, train loss: 0.356, epsilon: 0.027\n",
      "[time:27700]test score: -70.0, train loss: 0.358, epsilon: 0.026\n",
      "[time:27800]test score: -70.0, train loss: 0.410, epsilon: 0.026\n",
      "[time:27900]test score: -70.0, train loss: 0.341, epsilon: 0.026\n",
      "[time:28000]test score: -63.0, train loss: 0.507, epsilon: 0.026\n",
      "[time:28100]test score: -79.0, train loss: 0.683, epsilon: 0.026\n",
      "[time:28200]test score: -70.0, train loss: 0.419, epsilon: 0.026\n",
      "[time:28300]test score: -110.0, train loss: 0.329, epsilon: 0.026\n",
      "[time:28400]test score: -72.0, train loss: 0.824, epsilon: 0.026\n",
      "[time:28500]test score: -98.0, train loss: 0.327, epsilon: 0.025\n",
      "[time:28600]test score: -94.0, train loss: 0.538, epsilon: 0.025\n",
      "[time:28700]test score: -62.0, train loss: 0.379, epsilon: 0.025\n",
      "[time:28800]test score: -69.0, train loss: 0.358, epsilon: 0.025\n",
      "[time:28900]test score: -88.0, train loss: 0.514, epsilon: 0.025\n",
      "[time:29000]test score: -73.0, train loss: 0.466, epsilon: 0.025\n",
      "[time:29100]test score: -77.0, train loss: 0.496, epsilon: 0.025\n",
      "[time:29200]test score: -93.0, train loss: 0.549, epsilon: 0.024\n",
      "[time:29300]test score: -94.0, train loss: 0.412, epsilon: 0.024\n",
      "[time:29400]test score: -71.0, train loss: 0.360, epsilon: 0.024\n",
      "[time:29500]test score: -84.0, train loss: 0.410, epsilon: 0.024\n",
      "[time:29600]test score: -72.0, train loss: 0.386, epsilon: 0.024\n",
      "[time:29700]test score: -79.0, train loss: 0.519, epsilon: 0.024\n",
      "[time:29800]test score: -80.0, train loss: 0.433, epsilon: 0.023\n",
      "[time:29900]test score: -78.0, train loss: 0.819, epsilon: 0.023\n",
      "[time:30000]test score: -70.0, train loss: 0.490, epsilon: 0.023\n",
      "[time:30100]test score: -86.0, train loss: 0.450, epsilon: 0.023\n",
      "[time:30200]test score: -88.0, train loss: 0.497, epsilon: 0.023\n",
      "[time:30300]test score: -71.0, train loss: 0.484, epsilon: 0.023\n",
      "[time:30400]test score: -71.0, train loss: 0.631, epsilon: 0.023\n",
      "[time:30500]test score: -79.0, train loss: 0.492, epsilon: 0.023\n",
      "[time:30600]test score: -85.0, train loss: 0.406, epsilon: 0.022\n",
      "[time:30700]test score: -71.0, train loss: 0.440, epsilon: 0.022\n",
      "[time:30800]test score: -105.0, train loss: 0.523, epsilon: 0.022\n",
      "[time:30900]test score: -64.0, train loss: 0.331, epsilon: 0.022\n",
      "[time:31000]test score: -105.0, train loss: 0.402, epsilon: 0.022\n",
      "[time:31100]test score: -64.0, train loss: 0.421, epsilon: 0.022\n",
      "[time:31200]test score: -80.0, train loss: 0.396, epsilon: 0.022\n",
      "[time:31300]test score: -71.0, train loss: 0.448, epsilon: 0.021\n",
      "[time:31400]test score: -69.0, train loss: 0.515, epsilon: 0.021\n",
      "[time:31500]test score: -86.0, train loss: 0.560, epsilon: 0.021\n",
      "[time:31600]test score: -92.0, train loss: 0.459, epsilon: 0.021\n",
      "[time:31700]test score: -75.0, train loss: 0.638, epsilon: 0.021\n",
      "[time:31800]test score: -91.0, train loss: 0.478, epsilon: 0.021\n",
      "[time:31900]test score: -78.0, train loss: 0.363, epsilon: 0.021\n",
      "[time:32000]test score: -95.0, train loss: 0.666, epsilon: 0.021\n",
      "[time:32100]test score: -70.0, train loss: 0.667, epsilon: 0.020\n",
      "[time:32200]test score: -76.0, train loss: 0.546, epsilon: 0.020\n",
      "[time:32300]test score: -86.0, train loss: 0.314, epsilon: 0.020\n",
      "[time:32400]test score: -62.0, train loss: 0.396, epsilon: 0.020\n",
      "[time:32500]test score: -91.0, train loss: 0.523, epsilon: 0.020\n",
      "[time:32600]test score: -63.0, train loss: 0.425, epsilon: 0.020\n",
      "[time:32700]test score: -63.0, train loss: 0.579, epsilon: 0.020\n",
      "[time:32800]test score: -99.0, train loss: 0.545, epsilon: 0.020\n",
      "[time:32900]test score: -79.0, train loss: 0.317, epsilon: 0.019\n",
      "[time:33000]test score: -62.0, train loss: 0.567, epsilon: 0.019\n",
      "[time:33100]test score: -63.0, train loss: 0.656, epsilon: 0.019\n",
      "[time:33200]test score: -70.0, train loss: 0.598, epsilon: 0.019\n",
      "[time:33300]test score: -80.0, train loss: 0.499, epsilon: 0.019\n",
      "[time:33400]test score: -69.0, train loss: 0.583, epsilon: 0.019\n",
      "[time:33500]test score: -81.0, train loss: 0.537, epsilon: 0.019\n",
      "[time:33600]test score: -121.0, train loss: 0.305, epsilon: 0.019\n",
      "[time:33700]test score: -69.0, train loss: 0.410, epsilon: 0.019\n",
      "[time:33800]test score: -70.0, train loss: 0.385, epsilon: 0.018\n",
      "[time:33900]test score: -79.0, train loss: 0.570, epsilon: 0.018\n",
      "[time:34000]test score: -70.0, train loss: 0.625, epsilon: 0.018\n",
      "[time:34100]test score: -71.0, train loss: 0.513, epsilon: 0.018\n",
      "[time:34200]test score: -94.0, train loss: 0.515, epsilon: 0.018\n",
      "[time:34300]test score: -63.0, train loss: 0.509, epsilon: 0.018\n",
      "[time:34400]test score: -99.0, train loss: 0.474, epsilon: 0.018\n",
      "[time:34500]test score: -69.0, train loss: 0.803, epsilon: 0.018\n",
      "[time:34600]test score: -126.0, train loss: 0.603, epsilon: 0.018\n",
      "[time:34700]test score: -74.0, train loss: 0.460, epsilon: 0.017\n",
      "[time:34800]test score: -63.0, train loss: 0.578, epsilon: 0.017\n",
      "[time:34900]test score: -79.0, train loss: 0.574, epsilon: 0.017\n",
      "[time:35000]test score: -87.0, train loss: 0.410, epsilon: 0.017\n",
      "[time:35100]test score: -69.0, train loss: 0.368, epsilon: 0.017\n",
      "[time:35200]test score: -70.0, train loss: 0.530, epsilon: 0.017\n",
      "[time:35300]test score: -79.0, train loss: 0.640, epsilon: 0.017\n",
      "[time:35400]test score: -80.0, train loss: 0.706, epsilon: 0.017\n",
      "[time:35500]test score: -64.0, train loss: 0.566, epsilon: 0.017\n",
      "[time:35600]test score: -86.0, train loss: 0.333, epsilon: 0.016\n",
      "[time:35700]test score: -71.0, train loss: 0.501, epsilon: 0.016\n",
      "[time:35800]test score: -71.0, train loss: 0.509, epsilon: 0.016\n",
      "[time:35900]test score: -76.0, train loss: 0.881, epsilon: 0.016\n",
      "[time:36000]test score: -75.0, train loss: 0.707, epsilon: 0.016\n",
      "[time:36100]test score: -71.0, train loss: 0.862, epsilon: 0.016\n",
      "[time:36200]test score: -105.0, train loss: 0.629, epsilon: 0.016\n",
      "[time:36300]test score: -76.0, train loss: 0.379, epsilon: 0.016\n",
      "[time:36400]test score: -75.0, train loss: 0.388, epsilon: 0.016\n",
      "[time:36500]test score: -69.0, train loss: 0.310, epsilon: 0.016\n",
      "[time:36600]test score: -88.0, train loss: 0.454, epsilon: 0.015\n",
      "[time:36700]test score: -62.0, train loss: 0.590, epsilon: 0.015\n",
      "[time:36800]test score: -75.0, train loss: 0.562, epsilon: 0.015\n",
      "[time:36900]test score: -70.0, train loss: 0.426, epsilon: 0.015\n",
      "[time:37000]test score: -72.0, train loss: 0.552, epsilon: 0.015\n",
      "[time:37100]test score: -80.0, train loss: 0.661, epsilon: 0.015\n",
      "[time:37200]test score: -121.0, train loss: 0.587, epsilon: 0.015\n",
      "[time:37300]test score: -89.0, train loss: 0.628, epsilon: 0.015\n",
      "[time:37400]test score: -62.0, train loss: 0.553, epsilon: 0.015\n",
      "[time:37500]test score: -62.0, train loss: 0.700, epsilon: 0.015\n",
      "[time:37600]test score: -62.0, train loss: 0.522, epsilon: 0.015\n",
      "[time:37700]test score: -74.0, train loss: 0.445, epsilon: 0.014\n",
      "[time:37800]test score: -73.0, train loss: 0.636, epsilon: 0.014\n",
      "[time:37900]test score: -102.0, train loss: 0.433, epsilon: 0.014\n",
      "[time:38000]test score: -106.0, train loss: 0.460, epsilon: 0.014\n",
      "[time:38100]test score: -83.0, train loss: 0.692, epsilon: 0.014\n",
      "[time:38200]test score: -91.0, train loss: 0.506, epsilon: 0.014\n",
      "[time:38300]test score: -71.0, train loss: 1.214, epsilon: 0.014\n",
      "[time:38400]test score: -500.0, train loss: 0.599, epsilon: 0.014\n",
      "[time:38500]test score: -79.0, train loss: 0.808, epsilon: 0.014\n",
      "[time:38600]test score: -100.0, train loss: 1.408, epsilon: 0.014\n",
      "[time:38700]test score: -70.0, train loss: 0.850, epsilon: 0.014\n",
      "[time:38800]test score: -127.0, train loss: 1.021, epsilon: 0.014\n",
      "[time:38900]test score: -78.0, train loss: 0.649, epsilon: 0.014\n",
      "[time:39000]test score: -70.0, train loss: 0.550, epsilon: 0.014\n",
      "[time:39100]test score: -70.0, train loss: 0.454, epsilon: 0.013\n",
      "[time:39200]test score: -62.0, train loss: 0.488, epsilon: 0.013\n",
      "[time:39300]test score: -69.0, train loss: 0.718, epsilon: 0.013\n",
      "[time:39400]test score: -129.0, train loss: 0.710, epsilon: 0.013\n",
      "[time:39500]test score: -70.0, train loss: 0.811, epsilon: 0.013\n",
      "[time:39600]test score: -62.0, train loss: 0.649, epsilon: 0.013\n",
      "[time:39700]test score: -63.0, train loss: 0.767, epsilon: 0.013\n",
      "[time:39800]test score: -110.0, train loss: 1.407, epsilon: 0.013\n",
      "[time:39900]test score: -62.0, train loss: 0.822, epsilon: 0.013\n",
      "[time:40000]test score: -113.0, train loss: 0.657, epsilon: 0.013\n",
      "[time:40100]test score: -62.0, train loss: 0.734, epsilon: 0.013\n",
      "[time:40200]test score: -62.0, train loss: 0.600, epsilon: 0.012\n",
      "[time:40300]test score: -99.0, train loss: 0.496, epsilon: 0.012\n",
      "[time:40400]test score: -69.0, train loss: 0.289, epsilon: 0.012\n",
      "[time:40500]test score: -71.0, train loss: 0.521, epsilon: 0.012\n",
      "[time:40600]test score: -69.0, train loss: 0.716, epsilon: 0.012\n",
      "[time:40700]test score: -141.0, train loss: 0.492, epsilon: 0.012\n",
      "[time:40800]test score: -84.0, train loss: 0.881, epsilon: 0.012\n",
      "[time:40900]test score: -88.0, train loss: 0.824, epsilon: 0.012\n",
      "[time:41000]test score: -90.0, train loss: 0.592, epsilon: 0.012\n",
      "[time:41100]test score: -76.0, train loss: 0.342, epsilon: 0.012\n",
      "[time:41200]test score: -62.0, train loss: 0.778, epsilon: 0.012\n",
      "[time:41300]test score: -64.0, train loss: 0.436, epsilon: 0.012\n",
      "[time:41400]test score: -92.0, train loss: 0.425, epsilon: 0.012\n",
      "[time:41500]test score: -72.0, train loss: 0.734, epsilon: 0.011\n",
      "[time:41600]test score: -79.0, train loss: 0.842, epsilon: 0.011\n",
      "[time:41700]test score: -68.0, train loss: 0.763, epsilon: 0.011\n",
      "[time:41800]test score: -77.0, train loss: 0.872, epsilon: 0.011\n",
      "[time:41900]test score: -70.0, train loss: 0.787, epsilon: 0.011\n",
      "[time:42000]test score: -81.0, train loss: 1.101, epsilon: 0.011\n",
      "[time:42100]test score: -70.0, train loss: 1.085, epsilon: 0.011\n",
      "[time:42200]test score: -70.0, train loss: 0.804, epsilon: 0.011\n",
      "[time:42300]test score: -70.0, train loss: 0.421, epsilon: 0.011\n",
      "[time:42400]test score: -62.0, train loss: 0.362, epsilon: 0.011\n",
      "[time:42500]test score: -69.0, train loss: 0.694, epsilon: 0.011\n",
      "[time:42600]test score: -96.0, train loss: 0.816, epsilon: 0.011\n",
      "[time:42700]test score: -70.0, train loss: 0.492, epsilon: 0.010\n",
      "[time:42800]test score: -63.0, train loss: 0.291, epsilon: 0.010\n",
      "[time:42900]test score: -71.0, train loss: 0.565, epsilon: 0.010\n",
      "[time:43000]test score: -70.0, train loss: 0.542, epsilon: 0.010\n",
      "[time:43100]test score: -89.0, train loss: 0.359, epsilon: 0.010\n",
      "[time:43200]test score: -68.0, train loss: 0.490, epsilon: 0.010\n",
      "[time:43300]test score: -62.0, train loss: 0.824, epsilon: 0.010\n",
      "[time:43400]test score: -62.0, train loss: 0.402, epsilon: 0.010\n",
      "[time:43500]test score: -98.0, train loss: 0.740, epsilon: 0.010\n",
      "[time:43600]test score: -70.0, train loss: 0.416, epsilon: 0.010\n",
      "[time:43700]test score: -62.0, train loss: 0.655, epsilon: 0.010\n",
      "[time:43800]test score: -61.0, train loss: 0.670, epsilon: 0.010\n",
      "[time:43900]test score: -69.0, train loss: 0.542, epsilon: 0.010\n",
      "[time:44000]test score: -70.0, train loss: 0.283, epsilon: 0.010\n",
      "[time:44100]test score: -69.0, train loss: 0.693, epsilon: 0.010\n",
      "[time:44200]test score: -88.0, train loss: 0.421, epsilon: 0.009\n",
      "[time:44300]test score: -70.0, train loss: 0.692, epsilon: 0.009\n",
      "[time:44400]test score: -69.0, train loss: 0.693, epsilon: 0.009\n",
      "[time:44500]test score: -68.0, train loss: 0.452, epsilon: 0.009\n",
      "[time:44600]test score: -69.0, train loss: 0.528, epsilon: 0.009\n",
      "[time:44700]test score: -70.0, train loss: 0.635, epsilon: 0.009\n",
      "[time:44800]test score: -76.0, train loss: 0.675, epsilon: 0.009\n",
      "[time:44900]test score: -69.0, train loss: 0.344, epsilon: 0.009\n",
      "[time:45000]test score: -61.0, train loss: 1.196, epsilon: 0.009\n",
      "[time:45100]test score: -69.0, train loss: 0.687, epsilon: 0.009\n",
      "[time:45200]test score: -62.0, train loss: 0.921, epsilon: 0.009\n",
      "[time:45300]test score: -70.0, train loss: 0.929, epsilon: 0.009\n",
      "[time:45400]test score: -70.0, train loss: 0.569, epsilon: 0.009\n",
      "[time:45500]test score: -68.0, train loss: 0.841, epsilon: 0.009\n",
      "[time:45600]test score: -74.0, train loss: 0.482, epsilon: 0.009\n",
      "[time:45700]test score: -68.0, train loss: 0.655, epsilon: 0.009\n",
      "[time:45800]test score: -96.0, train loss: 0.614, epsilon: 0.009\n",
      "[time:45900]test score: -63.0, train loss: 0.633, epsilon: 0.009\n",
      "[time:46000]test score: -80.0, train loss: 0.436, epsilon: 0.008\n",
      "[time:46100]test score: -62.0, train loss: 0.422, epsilon: 0.008\n",
      "[time:46200]test score: -143.0, train loss: 0.493, epsilon: 0.008\n",
      "[time:46300]test score: -61.0, train loss: 0.470, epsilon: 0.008\n",
      "[time:46400]test score: -62.0, train loss: 0.631, epsilon: 0.008\n",
      "[time:46500]test score: -61.0, train loss: 0.513, epsilon: 0.008\n",
      "[time:46600]test score: -70.0, train loss: 0.492, epsilon: 0.008\n",
      "[time:46700]test score: -71.0, train loss: 0.597, epsilon: 0.008\n",
      "[time:46800]test score: -63.0, train loss: 0.403, epsilon: 0.008\n",
      "[time:46900]test score: -69.0, train loss: 0.633, epsilon: 0.008\n",
      "[time:47000]test score: -74.0, train loss: 0.506, epsilon: 0.008\n",
      "[time:47100]test score: -71.0, train loss: 0.248, epsilon: 0.008\n",
      "[time:47200]test score: -76.0, train loss: 0.560, epsilon: 0.008\n",
      "[time:47300]test score: -70.0, train loss: 0.457, epsilon: 0.008\n",
      "[time:47400]test score: -103.0, train loss: 0.444, epsilon: 0.008\n",
      "[time:47500]test score: -70.0, train loss: 0.664, epsilon: 0.008\n",
      "[time:47600]test score: -95.0, train loss: 0.642, epsilon: 0.008\n",
      "[time:47700]test score: -87.0, train loss: 0.653, epsilon: 0.008\n",
      "[time:47800]test score: -80.0, train loss: 0.669, epsilon: 0.008\n",
      "[time:47900]test score: -70.0, train loss: 0.613, epsilon: 0.007\n",
      "[time:48000]test score: -107.0, train loss: 0.857, epsilon: 0.007\n",
      "[time:48100]test score: -79.0, train loss: 0.800, epsilon: 0.007\n",
      "[time:48200]test score: -98.0, train loss: 0.699, epsilon: 0.007\n",
      "[time:48300]test score: -79.0, train loss: 0.598, epsilon: 0.007\n",
      "[time:48400]test score: -63.0, train loss: 0.704, epsilon: 0.007\n",
      "[time:48500]test score: -63.0, train loss: 0.592, epsilon: 0.007\n",
      "[time:48600]test score: -88.0, train loss: 0.350, epsilon: 0.007\n",
      "[time:48700]test score: -72.0, train loss: 0.378, epsilon: 0.007\n",
      "[time:48800]test score: -102.0, train loss: 0.442, epsilon: 0.007\n",
      "[time:48900]test score: -77.0, train loss: 0.631, epsilon: 0.007\n",
      "[time:49000]test score: -106.0, train loss: 0.613, epsilon: 0.007\n",
      "[time:49100]test score: -63.0, train loss: 0.669, epsilon: 0.007\n",
      "[time:49200]test score: -76.0, train loss: 0.642, epsilon: 0.007\n",
      "[time:49300]test score: -62.0, train loss: 0.358, epsilon: 0.007\n",
      "[time:49400]test score: -63.0, train loss: 0.540, epsilon: 0.007\n",
      "[time:49500]test score: -73.0, train loss: 0.568, epsilon: 0.007\n",
      "[time:49600]test score: -74.0, train loss: 0.378, epsilon: 0.007\n",
      "[time:49700]test score: -63.0, train loss: 0.636, epsilon: 0.007\n",
      "[time:49800]test score: -72.0, train loss: 0.597, epsilon: 0.007\n",
      "[time:49900]test score: -62.0, train loss: 0.324, epsilon: 0.007\n",
      "[time:50000]test score: -72.0, train loss: 0.370, epsilon: 0.007\n",
      "최종 점수: -68.0\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import gym\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = dict(\n",
    "    env_name            = \"Acrobot-v1\", # **환경은 변경 금지**\n",
    "    batch               = 128,          # 배치 크기\n",
    "    buffer_size         = 10000,        # 버퍼의 최대 크기(transition 단위)\n",
    "    lr                  = 0.001,        # 학습률\n",
    "    gamma               = 0.99,         # discount(할인율)\n",
    "    epsilon_init        = 0.1,          # 초기 epsilon(=초기 탐험 비율)\n",
    "    epsilon_min         = 0.001,        # 최소 epsilon(=최종 탐험 비율)\n",
    "    epsilon_decay       = 0.995,        # epsilon 감소 비율(매 episode마다 epsilon에 곱해짐)\n",
    "    n_step              = 50000,        # 학습 횟수 (time step 단위)\n",
    "    n_train_start       = 8000,         # 초기 버퍼 채우기(time step 단위)\n",
    "    target_update_freq  = 100,          # target Q의 업데이트 주기(time step 단위)\n",
    "    test_freq           = 100,          # test 주기(time step 단위)\n",
    ")\n",
    "\n",
    "env      = gym.make(config[\"env_name\"])\n",
    "test_env = gym.make(config[\"env_name\"])\n",
    "dState = env.observation_space.shape[0]\n",
    "dAction = env.action_space.n\n",
    "\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, dState, dAction):\n",
    "        super().__init__()\n",
    "        self.featureLayer = nn.Sequential(\n",
    "            # --- Q1: 입력층 ---\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.valueLayer = nn.Sequential(\n",
    "            # --- Q2: value 네트워크의 입력층 ---\n",
    "            nn.ReLU(),\n",
    "            # --- Q3: value 네트워크의 출력층 ---\n",
    "        )\n",
    "        self.advantageLayer = nn.Sequential(\n",
    "            # --- Q4: advantage 네트워크의 입력층 ---\n",
    "            nn.ReLU(),\n",
    "            # --- Q5: advantage 네트워크의 출력층 ---\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature = # --- Q6: 공통네트워크로 feature 계산 ---\n",
    "        value = # --- Q7: value 네트워크로 value 계산 ---\n",
    "        advantage = # --- Q8: advantage 네트워크로 advantage 계산 ---\n",
    "        Q = # --- Q9: 최종 Q 계산 ---\n",
    "        return Q\n",
    "\n",
    "Q = Qnet(dState=dState, dAction=dAction).to(DEVICE)\n",
    "targetQ = Qnet(dState=dState, dAction=dAction).to(DEVICE)\n",
    "targetQ.load_state_dict(Q.state_dict())\n",
    "optimizerQ = optim.Adam(Q.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "replay_buffer = deque(maxlen=config[\"buffer_size\"])\n",
    "\n",
    "gamma = config[\"gamma\"]\n",
    "epsilon = config[\"epsilon_init\"]\n",
    "\n",
    "def getAction(state, dAction, epsilon, Q):\n",
    "    with torch.no_grad():\n",
    "        if (random.random() > epsilon):\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "            action = Q(state).argmax(dim=-1).detach().cpu().numpy()\n",
    "        else:\n",
    "            action = np.random.randint(low=0, high=dAction)\n",
    "    return action\n",
    "\n",
    "def test(env, Q, dAction, epsilon):\n",
    "    with torch.no_grad():\n",
    "        score = 0.0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = getAction(state, dAction, epsilon, Q)\n",
    "            nextState, reward, done, _info = env.step(action)\n",
    "            score += reward\n",
    "            state = nextState\n",
    "            if done:\n",
    "                break\n",
    "    return score\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(1, config[\"n_train_start\"]+1):\n",
    "    action = getAction(state, dAction, epsilon, Q)\n",
    "    nextState, reward, done, _info = env.step(action)\n",
    "    transition = (state, action, reward, nextState, done)\n",
    "    replay_buffer.append(transition)\n",
    "    state = nextState\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "\n",
    "score = 0.0\n",
    "scores2 = []\n",
    "test_scores2 = []\n",
    "losses2 = []\n",
    "qs2 = []\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(1, config[\"n_step\"]+1):\n",
    "    action = getAction(state, dAction, epsilon, Q)\n",
    "    nextState, reward, done, _info = env.step(action)\n",
    "    transition = (state, action, reward, nextState, done)\n",
    "    replay_buffer.append(transition)\n",
    "\n",
    "    score += reward\n",
    "    state = nextState\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        scores2.append(score)\n",
    "        score = 0.0\n",
    "        epsilon = max(config[\"epsilon_min\"], epsilon*config[\"epsilon_decay\"])\n",
    "\n",
    "    transitions = random.sample(replay_buffer, config[\"batch\"])\n",
    "    batch = []\n",
    "    for item in zip(*transitions):\n",
    "        item = torch.from_numpy(np.stack(item)).float().to(DEVICE)\n",
    "        batch.append(item)\n",
    "    states, actions, rewards, nextStates, dones = batch\n",
    "    actions = actions.unsqueeze(dim=-1).long()\n",
    "    rewards = rewards.unsqueeze(dim=-1)\n",
    "    dones = dones.unsqueeze(dim=-1)\n",
    "\n",
    "    estimateQs = Q(states).gather(dim=1, index=actions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nextTargetQs = targetQ(nextStates).max(dim=-1, keepdim=True).values\n",
    "        targetQs = rewards + (1 - dones) * gamma * nextTargetQs\n",
    "\n",
    "    loss = F.mse_loss(targetQs.detach(), estimateQs)\n",
    "\n",
    "    optimizerQ.zero_grad()\n",
    "    # --- Q10: loss로 부터 역전파 수행 ---\n",
    "    optimizerQ.step()\n",
    "    losses2.append(loss.detach().cpu().item())\n",
    "    qs2.append(estimateQs.detach().mean().cpu().item())\n",
    "\n",
    "    if t % config[\"target_update_freq\"] == 0:\n",
    "        targetQ.load_state_dict(Q.state_dict())\n",
    "\n",
    "    if t % config[\"test_freq\"] == 0:\n",
    "        test_score = test(test_env, Q, dAction, epsilon=config[\"epsilon_min\"])\n",
    "        test_scores2.append(test_score)\n",
    "        print(f'[time:{t}]test score: {test_score}, train loss: {losses2[-1]:.3f}, epsilon: {epsilon:.3f}')\n",
    "\n",
    "print(f'최종 점수: {np.array(test_scores2[-10:]).mean()}')\n",
    "with open(\"result.csv\", \"w\") as f:\n",
    "    f.write(str(np.array(test_scores2[-10:]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Smk75Nw_XP24"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"elice_grade result.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
